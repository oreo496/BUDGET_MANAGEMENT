{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Global imports ready & modules reloaded\n"
     ]
    }
   ],
   "source": [
    "# Global Imports + Module Reload\n",
    "import importlib\n",
    "import os, time, json, warnings\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "print(\"[SUCCESS] Global imports ready & modules reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Global imports ready & modules reloaded\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import warnings\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import src.intelligence, src.pipeline\n",
    "importlib.reload(src.intelligence)\n",
    "importlib.reload(src.pipeline)\n",
    "from src.config import AppConfig\n",
    "from src.pipeline import FinanceAIEngine\n",
    "from src.models import CategorizeModel, FraudDetectionModel, GoalTrackingModel\n",
    "from src.models_pytorch import TabularDataset\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"[SUCCESS] Global imports ready & modules reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] Data Loading: 57.5ms\n",
      "[TIME] Monthly Alerts: 2.5ms\n",
      "Monthly alerts triggered: 0\n",
      "[TIME] Goal Feasibility: 2.1ms\n",
      "Goal feasibility: {'status': 'ERR_INVALID_USER', 'user_id': 0, 'feasibility_score': None, 'message': 'User not found in transaction history'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Refactored FinanceAI Architecture - Orchestration\n",
    "cfg = AppConfig()\n",
    "engine = FinanceAIEngine(cfg)\n",
    "engine.load_data(cfg.train_dataset_path or 'final_train_dataset.csv')\n",
    "alerts = engine.run_monthly_alerts()\n",
    "print('Monthly alerts triggered:', alerts)\n",
    "sample_user = int(engine.data['user_id'].iloc[0]) if 'user_id' in engine.data.columns and len(engine.data)>0 else 0\n",
    "result = engine.goal_feasibility(sample_user, target_amount=5000, months_to_deadline=6)\n",
    "print('Goal feasibility:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rbib9bfDPDN",
    "outputId": "260015db-38d5-4b3f-bc87-4d295e153fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local directory: c:\\Users\\Shahin Lap\\Desktop\\test\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Directory Configuration\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    DATA_DIR = \"/content/drive/MyDrive/AI MODEL\"\n",
    "except:\n",
    "    DATA_DIR = os.getcwd()\n",
    "    print(f\"Using local directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd_v_9kGMIMs",
    "outputId": "8433b599-cf82-46fc-8dd5-982b99e340d7"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Install Required Packages\n",
    "!pip -q install pytorch-tabnet shap xgboost lightgbm catboost optuna imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MVYoDfUDPDW",
    "outputId": "5220e5d1-3dca-4823-818e-0db15a873604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARING DATA FOR TRAINING\n",
      "======================================================================\n",
      "\n",
      "[DATA SHAPE] Initial: 8000 rows x 10 columns\n",
      "[MEMORY] Initial memory usage: 1.72 MB\n",
      "\n",
      "[DROPPED] Removed 2 non-predictive columns\n",
      "[RETAINED] Feature columns: 8\n",
      "\n",
      "[SPLIT] Train: 6400 | Test: 1600 | Features: 8 | Classes: 2\n",
      "\n",
      "[FEATURES] After preprocessing: 19 total | Target classes: 2\n",
      "\n",
      "[MEMORY OPTIMIZATION]\n",
      "   Initial (raw): 1.72 MB\n",
      "   Final (tensors): 0.58 MB\n",
      "   Saved: 1.14 MB (66.3%)\n",
      "   ID columns removed: 0\n",
      "\n",
      "[DEVICE] Using: cpu\n",
      "[MODEL DIMS] Input: 19 | Classes: 2 | Batches: 100/25\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Configure data and prepare tensors for training\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING DATA FOR TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "Y = engine.data['transaction_type'].copy() if 'transaction_type' in engine.data.columns else None\n",
    "if Y is None:\n",
    "    raise ValueError(\"Target column 'transaction_type' not found in data\")\n",
    "\n",
    "print(f\"\\n[DATA SHAPE] Initial: {engine.data.shape[0]} rows x {engine.data.shape[1]} columns\")\n",
    "initial_memory_mb = engine.data.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"[MEMORY] Initial memory usage: {initial_memory_mb:.2f} MB\")\n",
    "\n",
    "id_cols = ['transaction_id', 'user_id', 'account_id']\n",
    "metadata_cols = ['merchant_name', 'transaction_date', 'notes']\n",
    "target_cols = ['transaction_type', 'is_flagged']\n",
    "internal_cols = ['_is_train']\n",
    "exclude_cols = id_cols + metadata_cols + target_cols + internal_cols\n",
    "feature_cols = [c for c in engine.data.columns if c not in exclude_cols]\n",
    "removed_cols = set(engine.data.columns) & set(exclude_cols)\n",
    "if removed_cols:\n",
    "    print(f\"\\n[DROPPED] Removed {len(removed_cols)} non-predictive columns\")\n",
    "    print(f\"[RETAINED] Feature columns: {len(feature_cols)}\")\n",
    "\n",
    "numeric_features = [col for col in feature_cols if engine.data[col].dtype in ['int64', 'float64', 'float32']]\n",
    "categorical_features = [col for col in feature_cols if col not in numeric_features]\n",
    "X = engine.data[feature_cols]\n",
    "if '_is_train' in engine.data.columns:\n",
    "    X_train = X[engine.data['_is_train']].copy()\n",
    "    X_test = X[~engine.data['_is_train']].copy()\n",
    "    Y_train = Y[engine.data['_is_train']].copy()\n",
    "    Y_test = Y[~engine.data['_is_train']].copy()\n",
    "else:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "print(f\"\\n[SPLIT] Train: {X_train.shape[0]} | Test: {X_test.shape[0]} | Features: {X_train.shape[1]} | Classes: {Y.nunique()}\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "Y_test_encoded = label_encoder.transform(Y_test)\n",
    "\n",
    "numeric_features_list = [col for col in numeric_features if col in X_train.columns]\n",
    "categorical_features_list = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
    "categorical_features_list = [col for col in categorical_features_list if col not in ['_date_parsed', 'transaction_date']]\n",
    "\n",
    "X_train_numeric = X_train[numeric_features_list].fillna(X_train[numeric_features_list].median())\n",
    "X_test_numeric = X_test[numeric_features_list].fillna(X_test[numeric_features_list].median())\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_numeric)\n",
    "X_test_scaled = scaler.transform(X_test_numeric)\n",
    "\n",
    "if len(categorical_features_list) > 0:\n",
    "    X_train_cat = X_train[categorical_features_list].fillna('missing')\n",
    "    X_test_cat = X_test[categorical_features_list].fillna('missing')\n",
    "    cat_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    X_train_cat_enc = cat_encoder.fit_transform(X_train_cat)\n",
    "    X_test_cat_enc = cat_encoder.transform(X_test_cat)\n",
    "    X_train_final = np.concatenate([X_train_scaled, X_train_cat_enc], axis=1)\n",
    "    X_test_final = np.concatenate([X_test_scaled, X_test_cat_enc], axis=1)\n",
    "else:\n",
    "    X_train_final = X_train_scaled\n",
    "    X_test_final = X_test_scaled\n",
    "print(f\"\\n[FEATURES] After preprocessing: {X_train_final.shape[1]} total | Target classes: {len(np.unique(Y_train_encoded))}\")\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_final)\n",
    "Y_train_tensor = torch.LongTensor(Y_train_encoded)\n",
    "X_test_tensor = torch.FloatTensor(X_test_final)\n",
    "Y_test_tensor = torch.LongTensor(Y_test_encoded)\n",
    "\n",
    "final_memory_mb = X_train_tensor.element_size() * X_train_tensor.nelement() / (1024**2)\n",
    "final_memory_mb += X_test_tensor.element_size() * X_test_tensor.nelement() / (1024**2)\n",
    "memory_saved_mb = max(0, initial_memory_mb - final_memory_mb)\n",
    "memory_saved_pct = (memory_saved_mb / initial_memory_mb * 100) if initial_memory_mb > 0 else 0\n",
    "print(f\"\\n[MEMORY OPTIMIZATION]\")\n",
    "print(f\"   Initial (raw): {initial_memory_mb:.2f} MB\")\n",
    "print(f\"   Final (tensors): {final_memory_mb:.2f} MB\")\n",
    "print(f\"   Saved: {memory_saved_mb:.2f} MB ({memory_saved_pct:.1f}%)\")\n",
    "print(f\"   ID columns removed: {len([c for c in removed_cols if c in id_cols])}\")\n",
    "\n",
    "train_dataset = TabularDataset(X_train_tensor, Y_train_tensor, augment=True, noise_std=0.02)\n",
    "test_dataset = TabularDataset(X_test_tensor, Y_test_tensor, augment=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n[DEVICE] Using: {device}\")\n",
    "input_dim = X_train_final.shape[1]\n",
    "num_classes = len(np.unique(Y_train_encoded))\n",
    "num_epochs, patience_limit = 50, 10\n",
    "print(f\"[MODEL DIMS] Input: {input_dim} | Classes: {num_classes} | Batches: {len(train_loader)}/{len(test_loader)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ANOMALY DETECTION & FRAUD ALERT SYSTEM\n",
      "======================================================================\n",
      "\n",
      "[1/4] Preparing transaction data for anomaly detection...\n",
      "Prepared 8000 transactions for anomaly detection\n",
      "\n",
      "[2/4] Running Isolation Forest anomaly detection...\n",
      "Isolation Forest detected 800 anomalies\n",
      "\n",
      "[3/4] Applying Z-score statistical guardrails...\n",
      "Z-score analysis flagged 0 suspicious transactions\n",
      "\n",
      "[4/4] Generating explainable reasons for flagged transactions...\n",
      "\n",
      "======================================================================\n",
      "CREATING FRAUD ALERTS LOG FOR ADMIN REVIEW\n",
      "======================================================================\n",
      "\n",
      "[OK] Created fraud_alerts_log with 3152 flagged transactions\n",
      "\n",
      "Severity breakdown:\n",
      "{'LOW': 2352, 'MEDIUM': 800}\n",
      "\n",
      "======================================================================\n",
      "ANOMALY DETECTION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Total transactions analyzed: 8000\n",
      "Total flagged transactions: 3152 (39.4%)\n",
      "  - Z-score anomalies: 0\n",
      "  - Isolation Forest anomalies: 800\n",
      "\n",
      "INFO Sample flagged transactions with explanations:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Transaction ID: 2\n",
      "  Severity: LOW\n",
      "  Amount: $629.86 | Category: Health\n",
      "  Reason: Budget Overrun: You are 127% over budget in this category\n",
      "\n",
      "Transaction ID: 3\n",
      "  Severity: LOW\n",
      "  Amount: $424.38 | Category: Food\n",
      "  Reason: Budget Overrun: You are 113% over budget in this category\n",
      "\n",
      "Transaction ID: 5\n",
      "  Severity: LOW\n",
      "  Amount: $6.51 | Category: Shopping\n",
      "  Reason: Budget Overrun: You are 128% over budget in this category\n",
      "\n",
      "Transaction ID: 11\n",
      "  Severity: LOW\n",
      "  Amount: $353.12 | Category: Food\n",
      "  Reason: Budget Overrun: You are 138% over budget in this category\n",
      "\n",
      "Transaction ID: 12\n",
      "  Severity: MEDIUM\n",
      "  Amount: $4944.94 | Category: Salary\n",
      "  Reason: Behavioral Anomaly: Transaction pattern deviates from your normal spending behavior (Anomaly score: -0.676) | Budget Overrun: You are 119% over budget in this category\n",
      "\n",
      "[OK] Anomaly detection, statistical guardrails, and admin logging complete!\n",
      "[OK] Review 'fraud_alerts_log' dataframe for admin dashboard integration\n"
     ]
    }
   ],
   "source": [
    "# Anomaly Detection, Statistical Guardrails, Explainability & Admin Logging\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANOMALY DETECTION & FRAUD ALERT SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data_clean = engine.data.copy() if 'engine' in locals() else None\n",
    "if data_clean is None:\n",
    "    raise ValueError(\"engine.data is not available; run the earlier cells to load data.\")\n",
    "if '_is_train' not in data_clean.columns:\n",
    "    data_clean['_is_train'] = False\n",
    "\n",
    "print(\"\\n[1/4] Preparing transaction data for anomaly detection...\")\n",
    "transaction_data = data_clean[data_clean['_is_train'] == False].copy()\n",
    "transaction_data = transaction_data.reset_index(drop=True)\n",
    "if 'transaction_id' not in transaction_data.columns:\n",
    "    transaction_data['transaction_id'] = range(len(transaction_data))\n",
    "\n",
    "if 'amount' in transaction_data.columns:\n",
    "    amount_col = 'amount'\n",
    "elif 'transaction_amount' in transaction_data.columns:\n",
    "    amount_col = 'transaction_amount'\n",
    "else:\n",
    "    numeric_cols = transaction_data.select_dtypes(include=[np.number]).columns\n",
    "    amount_col = numeric_cols[0] if len(numeric_cols) > 0 else None\n",
    "\n",
    "if amount_col:\n",
    "    transaction_data['amount'] = transaction_data[amount_col].fillna(0)\n",
    "else:\n",
    "    transaction_data['amount'] = np.random.uniform(10, 500, len(transaction_data))\n",
    "\n",
    "if 'category' not in transaction_data.columns:\n",
    "    if 'transaction_type' in transaction_data.columns:\n",
    "        transaction_data['category'] = transaction_data['transaction_type']\n",
    "    else:\n",
    "        categories = ['Food', 'Transport', 'Entertainment', 'Shopping', 'Bills', 'Healthcare']\n",
    "        transaction_data['category'] = np.random.choice(categories, len(transaction_data))\n",
    "\n",
    "transaction_data['over_budget_percentage'] = np.random.uniform(0, 150, len(transaction_data))\n",
    "print(f\"Prepared {len(transaction_data)} transactions for anomaly detection\")\n",
    "\n",
    "print(\"\\n[2/4] Running Isolation Forest anomaly detection...\")\n",
    "anomaly_features = ['amount', 'over_budget_percentage']\n",
    "X_anomaly = transaction_data[anomaly_features].values\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
    "anomaly_predictions = iso_forest.fit_predict(X_anomaly)\n",
    "anomaly_scores = iso_forest.score_samples(X_anomaly)\n",
    "transaction_data['is_anomaly_if'] = (anomaly_predictions == -1).astype(int)\n",
    "transaction_data['anomaly_score_if'] = anomaly_scores\n",
    "print(f\"Isolation Forest detected {transaction_data['is_anomaly_if'].sum()} anomalies\")\n",
    "\n",
    "print(\"\\n[3/4] Applying Z-score statistical guardrails...\")\n",
    "\n",
    "def calculate_zscore_per_category(df, amount_col='amount', category_col='category'):\n",
    "    df = df.copy()\n",
    "    df['z_score'] = np.nan\n",
    "    for category in df[category_col].unique():\n",
    "        mask = df[category_col] == category\n",
    "        category_amounts = df.loc[mask, amount_col]\n",
    "        if len(category_amounts) > 1:\n",
    "            mean_amount = category_amounts.mean()\n",
    "            std_amount = category_amounts.std()\n",
    "            if std_amount > 0:\n",
    "                df.loc[mask, 'z_score'] = (category_amounts - mean_amount) / std_amount\n",
    "                df.loc[mask, 'category_mean'] = mean_amount\n",
    "                df.loc[mask, 'category_std'] = std_amount\n",
    "            else:\n",
    "                df.loc[mask, 'z_score'] = 0\n",
    "                df.loc[mask, 'category_mean'] = mean_amount\n",
    "                df.loc[mask, 'category_std'] = 0\n",
    "        else:\n",
    "            df.loc[mask, 'z_score'] = 0\n",
    "            df.loc[mask, 'category_mean'] = category_amounts.iloc[0] if len(category_amounts) > 0 else 0\n",
    "            df.loc[mask, 'category_std'] = 0\n",
    "    return df\n",
    "\n",
    "transaction_data = calculate_zscore_per_category(transaction_data)\n",
    "Z_SCORE_THRESHOLD = 3.0\n",
    "transaction_data['is_suspicious_zscore'] = (transaction_data['z_score'].abs() > Z_SCORE_THRESHOLD).astype(int)\n",
    "print(f\"Z-score analysis flagged {transaction_data['is_suspicious_zscore'].sum()} suspicious transactions\")\n",
    "\n",
    "print(\"\\n[4/4] Generating explainable reasons for flagged transactions...\")\n",
    "\n",
    "def generate_fraud_reason(row):\n",
    "    reasons = []\n",
    "    if row['is_suspicious_zscore'] == 1:\n",
    "        z_score = row['z_score']\n",
    "        mean_amt = row['category_mean']\n",
    "        percentage_diff = ((row['amount'] - mean_amt) / mean_amt * 100) if mean_amt > 0 else 0\n",
    "        reasons.append(\n",
    "            f\"Statistical Anomaly: This transaction is {abs(percentage_diff):.0f}% \"\n",
    "            f\"{'higher' if percentage_diff > 0 else 'lower'} than your average ${mean_amt:.2f} \"\n",
    "            f\"for the {row['category']} category (Z-score: {z_score:.2f})\"\n",
    "        )\n",
    "    if row['is_anomaly_if'] == 1:\n",
    "        reasons.append(\n",
    "            f\"Behavioral Anomaly: Transaction pattern deviates from your normal spending behavior \"\n",
    "            f\"(Anomaly score: {row['anomaly_score_if']:.3f})\"\n",
    "        )\n",
    "    if row['over_budget_percentage'] > 100:\n",
    "        reasons.append(\n",
    "            f\"Budget Overrun: You are {row['over_budget_percentage']:.0f}% over budget in this category\"\n",
    "        )\n",
    "    return ' | '.join(reasons) if reasons else 'No anomaly detected'\n",
    "\n",
    "transaction_data['fraud_reason'] = transaction_data.apply(generate_fraud_reason, axis=1)\n",
    "transaction_data['is_fraud_flagged'] = (\n",
    "    (transaction_data['is_suspicious_zscore'] == 1) | \n",
    "    (transaction_data['is_anomaly_if'] == 1) |\n",
    "    (transaction_data['over_budget_percentage'] > 100)\n",
    ").astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING FRAUD ALERTS LOG FOR ADMIN REVIEW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fraud_alerts_log = transaction_data[transaction_data['is_fraud_flagged'] == 1][\n",
    "    ['transaction_id', 'amount', 'category', 'z_score', 'anomaly_score_if', \n",
    "     'over_budget_percentage', 'fraud_reason']\n",
    "].copy()\n",
    "fraud_alerts_log['timestamp'] = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def determine_severity(row):\n",
    "    if row['is_suspicious_zscore'] == 1 and row['is_anomaly_if'] == 1:\n",
    "        return 'HIGH'\n",
    "    elif row['is_suspicious_zscore'] == 1 or row['is_anomaly_if'] == 1:\n",
    "        return 'MEDIUM'\n",
    "    else:\n",
    "        return 'LOW'\n",
    "\n",
    "fraud_alerts_log['severity'] = transaction_data[transaction_data['is_fraud_flagged'] == 1].apply(determine_severity, axis=1)\n",
    "fraud_alerts_log = fraud_alerts_log[\n",
    "    ['timestamp', 'transaction_id', 'severity', 'amount', 'category', \n",
    "     'z_score', 'anomaly_score_if', 'over_budget_percentage', 'fraud_reason']\n",
    "]\n",
    "\n",
    "print(f\"\\n[OK] Created fraud_alerts_log with {len(fraud_alerts_log)} flagged transactions\")\n",
    "print(f\"\\nSeverity breakdown:\")\n",
    "print(fraud_alerts_log['severity'].value_counts().to_dict())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANOMALY DETECTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "total_transactions = len(transaction_data)\n",
    "flagged_count = transaction_data['is_fraud_flagged'].sum()\n",
    "zscore_count = transaction_data['is_suspicious_zscore'].sum()\n",
    "isolation_count = transaction_data['is_anomaly_if'].sum()\n",
    "print(f\"\\nTotal transactions analyzed: {total_transactions}\")\n",
    "print(f\"Total flagged transactions: {flagged_count} ({flagged_count/total_transactions*100:.1f}%)\")\n",
    "print(f\"  - Z-score anomalies: {zscore_count}\")\n",
    "print(f\"  - Isolation Forest anomalies: {isolation_count}\")\n",
    "\n",
    "print(\"\\nINFO Sample flagged transactions with explanations:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in fraud_alerts_log.head(5).iterrows():\n",
    "    print(f\"\\nTransaction ID: {row['transaction_id']}\")\n",
    "    print(f\"  Severity: {row['severity']}\")\n",
    "    print(f\"  Amount: ${row['amount']:.2f} | Category: {row['category']}\")\n",
    "    print(f\"  Reason: {row['fraud_reason']}\")\n",
    "\n",
    "print(\"\\n[OK] Anomaly detection, statistical guardrails, and admin logging complete!\")\n",
    "print(\"[OK] Review 'fraud_alerts_log' dataframe for admin dashboard integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEEDBACK LOOP & AUDIT SYSTEM\n",
      "======================================================================\n",
      "\n",
      "[1/3] Initializing persistent feedback log system...\n",
      "[OK] Created feedback_log structure with 15 tracking fields\n",
      "\n",
      "[1.5/3] Initializing admin audit log system...\n",
      "[OK] Created ai_system_audit_log structure with 15 tracking fields\n",
      "\n",
      "[1.6/3] Implementing data validation & cold-start logic...\n",
      "[INFO] Created data validation & audit logging functions:\n",
      "  - validate_user_data()\n",
      "  - log_audit_entry()\n",
      "\n",
      "[2/3] Implementing feedback collection functions...\n",
      "[OK] Created feedback collection functions:\n",
      "  - log_prediction_feedback()\n",
      "  - log_category_prediction_feedback() [Multi-class category classifier]\n",
      "  - get_feedback_summary()\n",
      "  - process_feedback_for_retraining() [Category classifier support]\n",
      "  - prepare_category_training_data() [Training data preparation]\n"
     ]
    }
   ],
   "source": [
    "# Feedback & Audit Systems (Feedback Loop & Goal Tracking - Part 1/2)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEEDBACK LOOP & AUDIT SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[1/3] Initializing persistent feedback log system...\")\n",
    "\n",
    "def create_feedback_log_structure():\n",
    "    feedback_log = pd.DataFrame(columns=[\n",
    "        'feedback_id','timestamp','user_id','model_type','prediction_id','original_prediction',\n",
    "        'actual_outcome','feedback_type','user_explanation','model_confidence','corrected_value',\n",
    "        'impact_score','action_taken','resolved','notes'\n",
    "    ])\n",
    "    return feedback_log\n",
    "\n",
    "feedback_log = create_feedback_log_structure()\n",
    "print(f\"[OK] Created feedback_log structure with {len(feedback_log.columns)} tracking fields\")\n",
    "\n",
    "print(\"\\n[1.5/3] Initializing admin audit log system...\")\n",
    "\n",
    "def create_ai_system_audit_log_structure():\n",
    "    ai_system_audit_log = pd.DataFrame(columns=[\n",
    "        'audit_id','timestamp','user_id','decision_type','severity','trigger_reason',\n",
    "        'model_confidence_score','affected_transaction_id','affected_amount','affected_category',\n",
    "        'data_validation_status','model_used','admin_action_taken','resolution_notes','resolved'\n",
    "    ])\n",
    "    return ai_system_audit_log\n",
    "\n",
    "ai_system_audit_log = create_ai_system_audit_log_structure()\n",
    "print(f\"[OK] Created ai_system_audit_log structure with {len(ai_system_audit_log.columns)} tracking fields\")\n",
    "\n",
    "print(\"\\n[1.6/3] Implementing data validation & cold-start logic...\")\n",
    "\n",
    "STATUS_CODES = {\n",
    "    'SUCCESS': 'OK',\n",
    "    'ERR_INSUFFICIENT_DATA': 'ERR_INSUFFICIENT_DATA',\n",
    "    'ERR_MARGINAL_DATA': 'ERR_MARGINAL_DATA',\n",
    "    'ERR_INVALID_USER': 'ERR_INVALID_USER'\n",
    "}\n",
    "\n",
    "def validate_user_data(user_id, min_months=2):\n",
    "    user_transactions = data_clean[data_clean['user_id'] == user_id] if 'user_id' in data_clean.columns else None\n",
    "    if user_transactions is None or len(user_transactions) == 0:\n",
    "        return {\n",
    "            'status': STATUS_CODES['ERR_INVALID_USER'],\n",
    "            'user_id': user_id,\n",
    "            'months_available': 0,\n",
    "            'transaction_count': 0,\n",
    "            'is_valid': False,\n",
    "            'reason': 'User not found in transaction history'\n",
    "        }\n",
    "    date_col = None\n",
    "    for col in ['transaction_date', 'date', 'timestamp', 'created_at']:\n",
    "        if col in user_transactions.columns:\n",
    "            date_col = col\n",
    "            break\n",
    "    if date_col:\n",
    "        try:\n",
    "            user_transactions['_date_parsed'] = pd.to_datetime(user_transactions[date_col])\n",
    "            date_range = user_transactions['_date_parsed'].max() - user_transactions['_date_parsed'].min()\n",
    "            months_available = max(1, int(date_range.days / 30))\n",
    "        except:\n",
    "            months_available = 1\n",
    "    else:\n",
    "        months_available = max(1, len(user_transactions) // 10)\n",
    "    transaction_count = len(user_transactions)\n",
    "    if months_available >= min_months and transaction_count >= min_months * 5:\n",
    "        status = STATUS_CODES['SUCCESS']\n",
    "        is_valid = True\n",
    "    elif months_available >= 1 and transaction_count >= 5:\n",
    "        status = STATUS_CODES['ERR_MARGINAL_DATA']\n",
    "        is_valid = False\n",
    "    else:\n",
    "        status = STATUS_CODES['ERR_INSUFFICIENT_DATA']\n",
    "        is_valid = False\n",
    "    return {\n",
    "        'status': status,\n",
    "        'user_id': user_id,\n",
    "        'months_available': months_available,\n",
    "        'transaction_count': transaction_count,\n",
    "        'is_valid': is_valid,\n",
    "        'min_months_required': min_months,\n",
    "        'reason': 'Data validation passed' if is_valid else f'Only {months_available} month(s) available, {min_months} required'\n",
    "    }\n",
    "\n",
    "def log_audit_entry(user_id, decision_type, severity, trigger_reason, \n",
    "                   model_confidence_score=None, affected_transaction_id=None,\n",
    "                   affected_amount=None, affected_category=None, model_used=None):\n",
    "    global ai_system_audit_log\n",
    "    validation = validate_user_data(user_id, min_months=2)\n",
    "    data_status = validation['status'] if validation['status'] != STATUS_CODES['SUCCESS'] else 'SUFFICIENT_DATA'\n",
    "    audit_id = f\"audit_{len(ai_system_audit_log):08d}_{int(time.time())}\"\n",
    "    new_audit_entry = pd.DataFrame([{\n",
    "        'audit_id': audit_id,\n",
    "        'timestamp': datetime.now(timezone.utc).isoformat(),\n",
    "        'user_id': user_id,\n",
    "        'decision_type': decision_type,\n",
    "        'severity': severity,\n",
    "        'trigger_reason': trigger_reason,\n",
    "        'model_confidence_score': model_confidence_score,\n",
    "        'affected_transaction_id': affected_transaction_id,\n",
    "        'affected_amount': affected_amount,\n",
    "        'affected_category': affected_category,\n",
    "        'data_validation_status': data_status,\n",
    "        'model_used': model_used,\n",
    "        'admin_action_taken': 'pending',\n",
    "        'resolution_notes': '',\n",
    "        'resolved': False\n",
    "    }])\n",
    "    ai_system_audit_log = pd.concat([ai_system_audit_log, new_audit_entry], ignore_index=True)\n",
    "    return audit_id\n",
    "\n",
    "print(\"[INFO] Created data validation & audit logging functions:\")\n",
    "print(\"  - validate_user_data()\")\n",
    "print(\"  - log_audit_entry()\")\n",
    "\n",
    "print(\"\\n[2/3] Implementing feedback collection functions...\")\n",
    "\n",
    "def log_prediction_feedback(user_id, model_type, prediction_id, original_prediction, \n",
    "                           actual_outcome, feedback_type, user_explanation=\"\", \n",
    "                           model_confidence=None, corrected_value=None):\n",
    "    global feedback_log\n",
    "    feedback_id = f\"fb_{len(feedback_log):06d}_{int(time.time())}\"\n",
    "    feedback_impact_scores = {\n",
    "        'correct': 0.1,\n",
    "        'incorrect': 0.9,\n",
    "        'partially_correct': 0.5,\n",
    "        'reasoning_unclear': 0.4\n",
    "    }\n",
    "    impact_score = feedback_impact_scores.get(feedback_type, 0.5)\n",
    "    if model_confidence and feedback_type == 'incorrect':\n",
    "        impact_score = min(1.0, impact_score * (2.0 - model_confidence))\n",
    "    new_feedback = pd.DataFrame([{\n",
    "        'feedback_id': feedback_id,\n",
    "        'timestamp': datetime.now(timezone.utc).isoformat(),\n",
    "        'user_id': user_id,\n",
    "        'model_type': model_type,\n",
    "        'prediction_id': prediction_id,\n",
    "        'original_prediction': original_prediction,\n",
    "        'actual_outcome': actual_outcome,\n",
    "        'feedback_type': feedback_type,\n",
    "        'user_explanation': user_explanation,\n",
    "        'model_confidence': model_confidence,\n",
    "        'corrected_value': corrected_value,\n",
    "        'impact_score': impact_score,\n",
    "        'action_taken': 'log_only',\n",
    "        'resolved': False,\n",
    "        'notes': ''\n",
    "    }])\n",
    "    feedback_log = pd.concat([feedback_log, new_feedback], ignore_index=True)\n",
    "    return feedback_id\n",
    "\n",
    "def get_feedback_summary(model_type=None, days=30):\n",
    "    cutoff_date = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
    "    filtered_log = feedback_log[feedback_log['timestamp'] >= cutoff_date].copy()\n",
    "    if model_type:\n",
    "        filtered_log = filtered_log[filtered_log['model_type'] == model_type]\n",
    "    if len(filtered_log) == 0:\n",
    "        return {\n",
    "            'total_feedback_entries': 0,\n",
    "            'period_days': days,\n",
    "            'message': f'No feedback found for {model_type or \"any model\"} in past {days} days'\n",
    "        }\n",
    "    summary = {\n",
    "        'total_feedback_entries': len(filtered_log),\n",
    "        'period_days': days,\n",
    "        'feedback_breakdown': filtered_log['feedback_type'].value_counts().to_dict(),\n",
    "        'avg_impact_score': float(filtered_log['impact_score'].mean()),\n",
    "        'models_with_feedback': filtered_log['model_type'].unique().tolist(),\n",
    "        'high_impact_issues': len(filtered_log[filtered_log['impact_score'] > 0.75]),\n",
    "        'unresolved_feedback': len(filtered_log[filtered_log['resolved'] == False]),\n",
    "        'avg_model_confidence_when_wrong': float(\n",
    "            filtered_log[filtered_log['feedback_type'] == 'incorrect']['model_confidence'].mean()\n",
    "        ) if len(filtered_log[filtered_log['feedback_type'] == 'incorrect']) > 0 else None\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "def process_feedback_for_retraining(min_impact_threshold=0.75):\n",
    "    high_impact_feedback = feedback_log[(\n",
    "        (feedback_log['impact_score'] >= min_impact_threshold) & \n",
    "        (feedback_log['resolved'] == False)\n",
    "    )].copy()\n",
    "    if len(high_impact_feedback) == 0:\n",
    "        return {\n",
    "            'retrain_needed': False,\n",
    "            'message': 'No high-impact feedback requiring retraining'\n",
    "        }\n",
    "    retraining_by_model = high_impact_feedback.groupby('model_type').agg({\n",
    "        'impact_score': ['count', 'mean'],\n",
    "        'feedback_id': lambda x: x.tolist()\n",
    "    }).to_dict()\n",
    "    category_feedback = high_impact_feedback[high_impact_feedback['model_type'] == 'category_classifier']\n",
    "    category_corrections = []\n",
    "    if len(category_feedback) > 0:\n",
    "        for _, row in category_feedback.iterrows():\n",
    "            if row['corrected_value']:\n",
    "                category_corrections.append({\n",
    "                    'prediction_id': row['prediction_id'],\n",
    "                    'predicted_category': row['original_prediction'],\n",
    "                    'actual_category': row['corrected_value'],\n",
    "                    'user_explanation': row['user_explanation'],\n",
    "                    'impact_score': row['impact_score']\n",
    "                })\n",
    "    return {\n",
    "        'retrain_needed': True,\n",
    "        'high_impact_entries': len(high_impact_feedback),\n",
    "        'retraining_by_model': retraining_by_model,\n",
    "        'feedback_ids_for_review': high_impact_feedback['feedback_id'].tolist()[:10],\n",
    "        'category_classifier_corrections': category_corrections,\n",
    "        'recommendation': (\n",
    "            'Schedule retraining pipeline with feedback examples. '\n",
    "            f'Category classifier has {len(category_corrections)} corrections ready for multi-class retraining.'\n",
    "        ) if len(category_corrections) > 0 else 'Schedule retraining pipeline with these feedback examples'\n",
    "    }\n",
    "\n",
    "def log_category_prediction_feedback(user_id, transaction_id, predicted_category, \n",
    "                                    actual_category, model_confidence=None, \n",
    "                                    transaction_features=None):\n",
    "    user_explanation = f\"Category misprediction: '{predicted_category}' → '{actual_category}'.\"\n",
    "    if transaction_features:\n",
    "        details = []\n",
    "        if 'amount' in transaction_features:\n",
    "            details.append(f\"Amount: ${transaction_features['amount']:.2f}\")\n",
    "        if 'description' in transaction_features:\n",
    "            details.append(f\"Description: '{transaction_features['description']}'\")\n",
    "        if 'merchant' in transaction_features:\n",
    "            details.append(f\"Merchant: '{transaction_features['merchant']}'\")\n",
    "        if details:\n",
    "            user_explanation += \" Transaction: \" + \", \".join(details)\n",
    "    feedback_id = log_prediction_feedback(\n",
    "        user_id=user_id,\n",
    "        model_type='category_classifier',\n",
    "        prediction_id=transaction_id,\n",
    "        original_prediction=predicted_category,\n",
    "        actual_outcome=actual_category,\n",
    "        feedback_type='incorrect',\n",
    "        user_explanation=user_explanation,\n",
    "        model_confidence=model_confidence,\n",
    "        corrected_value=actual_category\n",
    "    )\n",
    "    return feedback_id\n",
    "\n",
    "def prepare_category_training_data(min_feedback_count=10):\n",
    "    category_feedback = feedback_log[(\n",
    "        (feedback_log['model_type'] == 'category_classifier') &\n",
    "        (feedback_log['corrected_value'].notna()) &\n",
    "        (feedback_log['feedback_type'] == 'incorrect')\n",
    "    )].copy()\n",
    "    if len(category_feedback) < min_feedback_count:\n",
    "        return {\n",
    "            'ready_for_training': False,\n",
    "            'feedback_count': len(category_feedback),\n",
    "            'min_required': min_feedback_count,\n",
    "            'message': f'Need {min_feedback_count - len(category_feedback)} more category corrections before retraining'\n",
    "        }\n",
    "    training_examples = []\n",
    "    for _, row in category_feedback.iterrows():\n",
    "        training_examples.append({\n",
    "            'transaction_id': row['prediction_id'],\n",
    "            'incorrect_prediction': row['original_prediction'],\n",
    "            'correct_label': row['corrected_value'],\n",
    "            'model_confidence': row['model_confidence'],\n",
    "            'impact_score': row['impact_score'],\n",
    "            'timestamp': row['timestamp']\n",
    "        })\n",
    "    correct_categories = category_feedback['corrected_value'].value_counts().to_dict()\n",
    "    incorrect_categories = category_feedback['original_prediction'].value_counts().to_dict()\n",
    "    confusion_pairs = [f\"{r['original_prediction']} → {r['corrected_value']}\" for _, r in category_feedback.iterrows()]\n",
    "    confusion_analysis = pd.Series(confusion_pairs).value_counts().head(10).to_dict()\n",
    "    return {\n",
    "        'ready_for_training': True,\n",
    "        'feedback_count': len(category_feedback),\n",
    "        'training_examples': training_examples,\n",
    "        'category_distribution': {\n",
    "            'correct_labels': correct_categories,\n",
    "            'misclassified_predictions': incorrect_categories\n",
    "        },\n",
    "        'confusion_pairs': confusion_analysis,\n",
    "        'avg_confidence_when_wrong': float(category_feedback['model_confidence'].mean()),\n",
    "        'high_confidence_errors': len(category_feedback[category_feedback['model_confidence'] > 0.8]),\n",
    "        'recommendation': (\n",
    "            f\"Retrain multi-class category classifier with {len(training_examples)} corrected examples. \"\n",
    "            f\"Focus on confusion pairs: {list(confusion_analysis.keys())[:3]}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "print(\"[OK] Created feedback collection functions:\")\n",
    "print(\"  - log_prediction_feedback()\")\n",
    "print(\"  - log_category_prediction_feedback() [Multi-class category classifier]\")\n",
    "print(\"  - get_feedback_summary()\")\n",
    "print(\"  - process_feedback_for_retraining() [Category classifier support]\")\n",
    "print(\"  - prepare_category_training_data() [Training data preparation]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GOAL TRACKING INTELLIGENCE SYSTEM\n",
      "======================================================================\n",
      "\n",
      "[3/3] Implementing goal tracking intelligence & savings projections...\n",
      "[OK] Created goal_tracking structure with 15 tracking fields\n",
      "[OK] Created goal tracking intelligence functions:\n",
      "  - calculate_savings_projection()\n",
      "  - track_goal_progress()\n",
      "  - generate_goal_recommendation()\n",
      "  - get_user_goal_summary()\n",
      "\n",
      "[4/4] Implementing goal feasibility analysis...\n",
      "[OK] Created goal feasibility analysis function:\n",
      "  - analyze_goal_feasibility(user_id, target_amount, months_to_deadline)\n",
      "\n",
      "======================================================================\n",
      "FEEDBACK, GOALS, VALIDATION & AUDIT DEMO\n",
      "======================================================================\n",
      "\n",
      "[DEMO] Logging sample feedback...\n",
      "\n",
      "[DEMO] Logging category prediction feedback (Multi-Class Classifier)...\n",
      "[OK] Logged 5 feedback entries (2 general + 3 category predictions)\n",
      "\n",
      "[DEMO] Feedback Summary:\n",
      "  Total entries: 5\n",
      "  Avg impact: 0.90\n",
      "\n",
      "[DEMO] Category Classifier Training Data:\n",
      "  [OK] Ready for retraining: 3 category corrections\n",
      "  [OK] Avg confidence when wrong: 0.73\n",
      "  [OK] High-confidence errors: 1\n",
      "  [OK] Top confusion pairs:\n",
      "     - Dining → Groceries: 1x\n",
      "     - Entertainment → Education: 1x\n",
      "     - Shopping → Healthcare: 1x\n",
      "  [OK] Recommendation: Retrain multi-class category classifier with 3 corrected examples. Focus on confusion pairs: ['Dining → Groceries', 'Entertainment → Education', 'Shopping → Healthcare']\n",
      "\n",
      "[DEMO] User Data Validation:\n",
      "  User 101 Status: ERR_INVALID_USER\n",
      "  Months Available: 0\n",
      "  Transaction Count: 0\n",
      "  Data Valid: False\n",
      "\n",
      "[DEMO] Goal Feasibility Analysis (with validation & audit):\n",
      "  [OK] Goal: $5000.00\n",
      "  [OK] Feasibility: 1.00 (100%)\n",
      "  [OK] Status: achievable\n",
      "  [OK] Monthly Net Savings: $7929.79\n",
      "  [OK] Projected Savings: $47578.76\n",
      "  [OK] Shortfall: $0.00\n",
      "\n",
      "[OK] Optimized Feedback Loop & Goal Tracking System READY!\n",
      "[OK] With Data Validation & Admin Audit Logging for Compliance\n",
      "[OK] Multi-Class Category Classifier Support with Corrected Training Data\n"
     ]
    }
   ],
   "source": [
    "# Goal Tracking, Feasibility Analysis & Demo (Feedback Loop & Goal Tracking - Part 2/2)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GOAL TRACKING INTELLIGENCE SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[3/3] Implementing goal tracking intelligence & savings projections...\")\n",
    "\n",
    "def create_goal_tracking_structure():\n",
    "    goal_tracking = pd.DataFrame(columns=[\n",
    "        'goal_id','user_id','goal_name','goal_amount','goal_deadline','current_savings',\n",
    "        'monthly_target','last_updated','projected_completion','completion_probability','status',\n",
    "        'monthly_savings_history','projection_data','alerts','milestones_met'\n",
    "    ])\n",
    "    return goal_tracking\n",
    "\n",
    "goal_tracking = create_goal_tracking_structure()\n",
    "print(f\"[OK] Created goal_tracking structure with {len(goal_tracking.columns)} tracking fields\")\n",
    "\n",
    "def calculate_savings_projection(goal_id, current_savings, goal_amount, \n",
    "                                monthly_contribution, months_remaining, \n",
    "                                historical_data=None):\n",
    "    projected_total = current_savings + (monthly_contribution * months_remaining)\n",
    "    months_needed = (goal_amount - current_savings) / max(monthly_contribution, 1)\n",
    "    completion_date = (datetime.now(timezone.utc) + timedelta(days=30*months_needed)).isoformat()\n",
    "    if historical_data and len(historical_data) > 1:\n",
    "        historical_array = np.array(historical_data)\n",
    "        contribution_mean = historical_array.mean()\n",
    "        contribution_std = historical_array.std()\n",
    "        lower_bound = projected_total - (1.96 * contribution_std * np.sqrt(months_remaining))\n",
    "        upper_bound = projected_total + (1.96 * contribution_std * np.sqrt(months_remaining))\n",
    "        z_score = (goal_amount - projected_total) / max(contribution_std * np.sqrt(months_remaining), 0.01)\n",
    "        completion_probability = float(stats.norm.cdf(-z_score))\n",
    "    else:\n",
    "        lower_bound = projected_total * 0.9\n",
    "        upper_bound = projected_total * 1.1\n",
    "        completion_probability = 1.0 if projected_total >= goal_amount else 0.5\n",
    "    if projected_total >= goal_amount:\n",
    "        status = 'on_track' if months_needed <= months_remaining else 'at_risk'\n",
    "    else:\n",
    "        status = 'off_track'\n",
    "    return {\n",
    "        'goal_id': goal_id,\n",
    "        'projected_total': float(projected_total),\n",
    "        'target_amount': goal_amount,\n",
    "        'shortfall': max(0, goal_amount - projected_total),\n",
    "        'surplus': max(0, projected_total - goal_amount),\n",
    "        'months_needed': float(months_needed),\n",
    "        'months_remaining': months_remaining,\n",
    "        'completion_date': completion_date,\n",
    "        'completion_probability': min(1.0, completion_probability),\n",
    "        'confidence_interval_95': {\n",
    "            'lower': float(lower_bound),\n",
    "            'upper': float(upper_bound)\n",
    "        },\n",
    "        'status': status,\n",
    "        'required_monthly_adjustment': float(\n",
    "            (goal_amount - current_savings) / months_remaining if months_remaining > 0 else 0\n",
    "        ),\n",
    "        'current_trajectory_monthly': float(monthly_contribution)\n",
    "    }\n",
    "\n",
    "def track_goal_progress(user_id, goal_id, goal_name, goal_amount, goal_deadline, \n",
    "                       current_savings, monthly_contributions_history=None):\n",
    "    global goal_tracking\n",
    "    deadline_dt = datetime.fromisoformat(goal_deadline.replace('Z', '+00:00'))\n",
    "    months_remaining = max(1, int((deadline_dt - datetime.now(timezone.utc)).days / 30))\n",
    "    if monthly_contributions_history:\n",
    "        avg_monthly = np.mean(monthly_contributions_history)\n",
    "        historical_std = np.std(monthly_contributions_history)\n",
    "    else:\n",
    "        avg_monthly = (goal_amount - current_savings) / max(months_remaining, 1)\n",
    "        historical_std = avg_monthly * 0.2\n",
    "        monthly_contributions_history = []\n",
    "    projection = calculate_savings_projection(\n",
    "        goal_id, current_savings, goal_amount, avg_monthly, \n",
    "        months_remaining, monthly_contributions_history\n",
    "    )\n",
    "    alerts = []\n",
    "    if projection['status'] == 'off_track':\n",
    "        alerts.append({\n",
    "            'level': 'warning',\n",
    "            'message': f\"Goal '{goal_name}' is off track. Need ${projection['required_monthly_adjustment']:.2f}/month instead of ${projection['current_trajectory_monthly']:.2f}\"\n",
    "        })\n",
    "    if projection['completion_probability'] < 0.5:\n",
    "        alerts.append({\n",
    "            'level': 'critical',\n",
    "            'message': \"Less than 50% probability of achieving goal. Consider extending deadline or increasing contributions.\"\n",
    "        })\n",
    "    milestones = [0.25, 0.5, 0.75, 1.0]\n",
    "    progress_percentage = (current_savings / goal_amount) * 100\n",
    "    milestones_met = sum(1 for m in milestones if progress_percentage >= m * 100)\n",
    "    goal_entry = {\n",
    "        'goal_id': goal_id,\n",
    "        'user_id': user_id,\n",
    "        'goal_name': goal_name,\n",
    "        'goal_amount': goal_amount,\n",
    "        'goal_deadline': goal_deadline,\n",
    "        'current_savings': current_savings,\n",
    "        'monthly_target': projection['required_monthly_adjustment'],\n",
    "        'last_updated': datetime.now(timezone.utc).isoformat(),\n",
    "        'projected_completion': projection['completion_date'],\n",
    "        'completion_probability': projection['completion_probability'],\n",
    "        'status': projection['status'],\n",
    "        'monthly_savings_history': json.dumps(monthly_contributions_history),\n",
    "        'projection_data': json.dumps({\n",
    "            'projected_total': projection['projected_total'],\n",
    "            'shortfall': projection['shortfall'],\n",
    "            'surplus': projection['surplus'],\n",
    "            'confidence_interval_95': projection['confidence_interval_95']\n",
    "        }),\n",
    "        'alerts': json.dumps(alerts),\n",
    "        'milestones_met': milestones_met\n",
    "    }\n",
    "    goal_entry_df = pd.DataFrame([goal_entry])\n",
    "    goal_tracking = pd.concat([goal_tracking, goal_entry_df], ignore_index=True)\n",
    "    return {\n",
    "        'goal_id': goal_id,\n",
    "        'goal_name': goal_name,\n",
    "        'progress_percentage': progress_percentage,\n",
    "        'current_savings': current_savings,\n",
    "        'goal_amount': goal_amount,\n",
    "        'projection': projection,\n",
    "        'alerts': alerts,\n",
    "        'milestones_met': milestones_met,\n",
    "        'recommendation': generate_goal_recommendation(projection, alerts)\n",
    "    }\n",
    "\n",
    "def generate_goal_recommendation(projection, alerts):\n",
    "    if projection['completion_probability'] > 0.9:\n",
    "        return \"OK: On excellent track! Maintain current savings rate.\"\n",
    "    elif projection['completion_probability'] > 0.7:\n",
    "        return \"OK: On good track. Consider small increases to finish ahead of schedule.\"\n",
    "    elif projection['completion_probability'] > 0.5:\n",
    "        return \"WARNING: Achievable but requires consistent effort. Increase monthly contributions if possible.\"\n",
    "    else:\n",
    "        adjustment_needed = projection['required_monthly_adjustment'] - projection['current_trajectory_monthly']\n",
    "        return f\"OFF TRACK: Need to increase savings by ${adjustment_needed:.2f}/month or extend deadline.\"\n",
    "\n",
    "def get_user_goal_summary(user_id):\n",
    "    user_goals = goal_tracking[goal_tracking['user_id'] == user_id]\n",
    "    if len(user_goals) == 0:\n",
    "        return {'user_id': user_id, 'total_goals': 0, 'goals': []}\n",
    "    total_target = user_goals['goal_amount'].sum()\n",
    "    total_saved = user_goals['current_savings'].sum()\n",
    "    overall_progress = (total_saved / total_target * 100) if total_target > 0 else 0\n",
    "    status_counts = user_goals['status'].value_counts()\n",
    "    goals_summary = [\n",
    "        {\n",
    "            'goal_name': goal['goal_name'],\n",
    "            'status': goal['status'],\n",
    "            'progress_percentage': (goal['current_savings'] / goal['goal_amount'] * 100),\n",
    "            'completion_probability': goal['completion_probability'],\n",
    "            'milestones_met': goal['milestones_met'],\n",
    "            'deadline': goal['goal_deadline'],\n",
    "            'alerts': json.loads(goal['alerts']) if isinstance(goal['alerts'], str) else goal['alerts']\n",
    "        }\n",
    "        for _, goal in user_goals.iterrows()\n",
    "    ]\n",
    "    return {\n",
    "        'user_id': user_id,\n",
    "        'total_goals': len(user_goals),\n",
    "        'overall_progress_percentage': overall_progress,\n",
    "        'total_target': total_target,\n",
    "        'total_saved': total_saved,\n",
    "        'total_shortfall': max(0, total_target - total_saved),\n",
    "        'goals': goals_summary,\n",
    "        'on_track_count': status_counts.get('on_track', 0),\n",
    "        'at_risk_count': status_counts.get('at_risk', 0),\n",
    "        'off_track_count': status_counts.get('off_track', 0)\n",
    "    }\n",
    "\n",
    "print(\"[OK] Created goal tracking intelligence functions:\")\n",
    "print(\"  - calculate_savings_projection()\")\n",
    "print(\"  - track_goal_progress()\")\n",
    "print(\"  - generate_goal_recommendation()\")\n",
    "print(\"  - get_user_goal_summary()\")\n",
    "\n",
    "print(\"\\n[4/4] Implementing goal feasibility analysis...\")\n",
    "\n",
    "def analyze_goal_feasibility(user_id, target_amount, months_to_deadline, category_filter=None):\n",
    "    validation = validate_user_data(user_id, min_months=2)\n",
    "    if validation['status'] == STATUS_CODES['ERR_INSUFFICIENT_DATA']:\n",
    "        audit_id = log_audit_entry(\n",
    "            user_id=user_id,\n",
    "            decision_type='goal_warning',\n",
    "            severity='LOW',\n",
    "            trigger_reason=f\"Insufficient data for goal feasibility analysis. Only {validation['months_available']} month(s) available, 2 required.\",\n",
    "            model_confidence_score=0.0,\n",
    "            model_used='analyze_goal_feasibility'\n",
    "        )\n",
    "        return {\n",
    "            'user_id': user_id,\n",
    "            'status': STATUS_CODES['ERR_INSUFFICIENT_DATA'],\n",
    "            'feasibility_score': None,\n",
    "            'message': 'Insufficient transaction history for reliable analysis',\n",
    "            'months_required': 2,\n",
    "            'months_available': validation['months_available'],\n",
    "            'audit_id': audit_id\n",
    "        }\n",
    "    if validation['status'] == STATUS_CODES['ERR_MARGINAL_DATA']:\n",
    "        print(f\"WARNING: Marginal data for user {user_id}. Analysis may be unreliable.\")\n",
    "    user_transactions = data_clean[data_clean['user_id'] == user_id] if 'user_id' in data_clean.columns else data_clean.head(100)\n",
    "    if len(user_transactions) == 0:\n",
    "        audit_id = log_audit_entry(\n",
    "            user_id=user_id,\n",
    "            decision_type='goal_warning',\n",
    "            severity='LOW',\n",
    "            trigger_reason='No transaction history found',\n",
    "            model_used='analyze_goal_feasibility'\n",
    "        )\n",
    "        return {\n",
    "            'user_id': user_id,\n",
    "            'status': STATUS_CODES['ERR_INVALID_USER'],\n",
    "            'feasibility_score': None,\n",
    "            'message': 'No transaction history found for user',\n",
    "            'audit_id': audit_id\n",
    "        }\n",
    "    numeric_cols = user_transactions.select_dtypes(include=[np.number]).columns\n",
    "    estimated_monthly_income = 0\n",
    "    estimated_monthly_expenses = 0\n",
    "    if len(numeric_cols) > 0:\n",
    "        amount_col = numeric_cols[0]\n",
    "        transactions = user_transactions[amount_col].dropna()\n",
    "        if len(transactions) > 0:\n",
    "            positive_amount = transactions[transactions > 0].mean() if len(transactions[transactions > 0]) > 0 else 0\n",
    "            negative_amount = abs(transactions[transactions < 0].mean()) if len(transactions[transactions < 0]) > 0 else 0\n",
    "            estimated_monthly_income = max(positive_amount * 10, 2000)\n",
    "            estimated_monthly_expenses = max(negative_amount * 10, 1500)\n",
    "    else:\n",
    "        estimated_monthly_income = 3500\n",
    "        estimated_monthly_expenses = 2200\n",
    "    avg_monthly_net_savings = max(0, estimated_monthly_income - estimated_monthly_expenses)\n",
    "    projected_savings = avg_monthly_net_savings * months_to_deadline\n",
    "    projected_shortfall = max(0, target_amount - projected_savings)\n",
    "    if projected_shortfall == 0:\n",
    "        feasibility_score = 1.0\n",
    "        status = 'achievable'\n",
    "    elif projected_shortfall <= target_amount * 0.2:\n",
    "        feasibility_score = 0.75\n",
    "        status = 'achievable'\n",
    "    elif projected_shortfall <= target_amount * 0.5:\n",
    "        feasibility_score = 0.5\n",
    "        status = 'challenging'\n",
    "    else:\n",
    "        feasibility_score = max(0.1, projected_savings / target_amount)\n",
    "        status = 'unrealistic'\n",
    "    category_breakdown = {}\n",
    "    if 'transaction_type' in user_transactions.columns:\n",
    "        type_counts = user_transactions['transaction_type'].value_counts()\n",
    "        category_breakdown = type_counts.head(5).to_dict()\n",
    "    monthly_reduction_needed = projected_shortfall / max(months_to_deadline, 1)\n",
    "    recommendations = []\n",
    "    if monthly_reduction_needed > 0:\n",
    "        categories = {\n",
    "            'Dining': {'avg': 250, 'reduction_pct': 0.20},\n",
    "            'Entertainment': {'avg': 150, 'reduction_pct': 0.25},\n",
    "            'Shopping': {'avg': 200, 'reduction_pct': 0.30},\n",
    "            'Transport': {'avg': 200, 'reduction_pct': 0.15}\n",
    "        }\n",
    "        remaining_reduction = monthly_reduction_needed\n",
    "        for category, details in categories.items():\n",
    "            if remaining_reduction <= 0:\n",
    "                break\n",
    "            reduction = min(remaining_reduction, details['avg'] * details['reduction_pct'])\n",
    "            if reduction > 0:\n",
    "                recommendations.append({\n",
    "                    'category': category,\n",
    "                    'current_estimate': details['avg'],\n",
    "                    'suggested_reduction': round(reduction, 2),\n",
    "                    'impact': f\"Save ${round(reduction * months_to_deadline, 2)} by deadline\"\n",
    "                })\n",
    "                remaining_reduction -= reduction\n",
    "    recommendations_text = \"Goal is within reach with current spending patterns\" if not recommendations else \" | \".join([f\"Reduce {r['category']} by ${r['suggested_reduction']:.2f}/month\" for r in recommendations])\n",
    "    audit_id = None\n",
    "    if status in ['challenging', 'unrealistic']:\n",
    "        audit_id = log_audit_entry(\n",
    "            user_id=user_id,\n",
    "            decision_type='goal_warning',\n",
    "            severity='MEDIUM' if status == 'challenging' else 'HIGH',\n",
    "            trigger_reason=f\"Goal status: {status}. Feasibility score: {feasibility_score:.2f}. Shortfall: ${projected_shortfall:.2f}\",\n",
    "            model_confidence_score=feasibility_score,\n",
    "            model_used='analyze_goal_feasibility'\n",
    "        )\n",
    "    return {\n",
    "        'status': STATUS_CODES['SUCCESS'],\n",
    "        'user_id': user_id,\n",
    "        'target_amount': target_amount,\n",
    "        'months_to_deadline': months_to_deadline,\n",
    "        'data_validation_status': validation['status'],\n",
    "        'feasibility_score': round(feasibility_score, 2),\n",
    "        'goal_status': status,\n",
    "        'avg_monthly_income': round(estimated_monthly_income, 2),\n",
    "        'avg_monthly_expenses': round(estimated_monthly_expenses, 2),\n",
    "        'avg_monthly_net_savings': round(avg_monthly_net_savings, 2),\n",
    "        'projected_savings': round(projected_savings, 2),\n",
    "        'projected_shortfall': round(projected_shortfall, 2),\n",
    "        'monthly_reduction_needed': round(monthly_reduction_needed, 2),\n",
    "        'category_breakdown': category_breakdown,\n",
    "        'recommended_adjustments': recommendations,\n",
    "        'recommended_adjustment_summary': recommendations_text,\n",
    "        'completion_confidence': f\"{feasibility_score*100:.0f}%\",\n",
    "        'audit_id': audit_id\n",
    "    }\n",
    "\n",
    "print(\"[OK] Created goal feasibility analysis function:\")\n",
    "print(\"  - analyze_goal_feasibility(user_id, target_amount, months_to_deadline)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEEDBACK, GOALS, VALIDATION & AUDIT DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[DEMO] Logging sample feedback...\")\n",
    "log_prediction_feedback(101, 'budget', 'pred_001', 'will_not_exceed_budget', \n",
    "                       'exceeded_budget_by_15_percent', 'incorrect',\n",
    "                       'Unexpected medical expense', model_confidence=0.87)\n",
    "log_prediction_feedback(102, 'goal', 'pred_002', 0.65, 0.78, 'partially_correct',\n",
    "                       'Model underestimated savings discipline', model_confidence=0.65)\n",
    "\n",
    "print(\"\\n[DEMO] Logging category prediction feedback (Multi-Class Classifier)...\")\n",
    "log_category_prediction_feedback(\n",
    "    user_id=101,\n",
    "    transaction_id='txn_12345',\n",
    "    predicted_category='Dining',\n",
    "    actual_category='Groceries',\n",
    "    model_confidence=0.82,\n",
    "    transaction_features={\n",
    "        'amount': 87.50,\n",
    "        'description': 'WHOLE FOODS MARKET',\n",
    "        'merchant': 'Whole Foods'\n",
    "    }\n",
    ")\n",
    "\n",
    "log_category_prediction_feedback(\n",
    "    user_id=102,\n",
    "    transaction_id='txn_67890',\n",
    "    predicted_category='Entertainment',\n",
    "    actual_category='Education',\n",
    "    model_confidence=0.65,\n",
    "    transaction_features={\n",
    "        'amount': 149.99,\n",
    "        'description': 'UDEMY COURSE PURCHASE',\n",
    "        'merchant': 'Udemy'\n",
    "    }\n",
    ")\n",
    "\n",
    "log_category_prediction_feedback(\n",
    "    user_id=103,\n",
    "    transaction_id='txn_11223',\n",
    "    predicted_category='Shopping',\n",
    "    actual_category='Healthcare',\n",
    "    model_confidence=0.71,\n",
    "    transaction_features={\n",
    "        'amount': 45.00,\n",
    "        'description': 'WALGREENS PHARMACY',\n",
    "        'merchant': 'Walgreens'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"[OK] Logged 5 feedback entries (2 general + 3 category predictions)\")\n",
    "\n",
    "print(\"\\n[DEMO] Feedback Summary:\")\n",
    "summary = get_feedback_summary()\n",
    "print(f\"  Total entries: {summary['total_feedback_entries']}\")\n",
    "if summary['total_feedback_entries'] > 0:\n",
    "    print(f\"  Avg impact: {summary['avg_impact_score']:.2f}\")\n",
    "\n",
    "print(\"\\n[DEMO] Category Classifier Training Data:\")\n",
    "category_training = prepare_category_training_data(min_feedback_count=2)\n",
    "if category_training['ready_for_training']:\n",
    "    print(f\"  [OK] Ready for retraining: {category_training['feedback_count']} category corrections\")\n",
    "    print(f\"  [OK] Avg confidence when wrong: {category_training['avg_confidence_when_wrong']:.2f}\")\n",
    "    print(f\"  [OK] High-confidence errors: {category_training['high_confidence_errors']}\")\n",
    "    print(f\"  [OK] Top confusion pairs:\")\n",
    "    for pair, count in list(category_training['confusion_pairs'].items())[:3]:\n",
    "        print(f\"     - {pair}: {count}x\")\n",
    "    print(f\"  [OK] Recommendation: {category_training['recommendation']}\")\n",
    "else:\n",
    "    print(f\"  WARNING Not ready: {category_training['feedback_count']}/{category_training['min_required']} corrections\")\n",
    "\n",
    "print(\"\\n[DEMO] User Data Validation:\")\n",
    "validation_result = validate_user_data(user_id=101, min_months=2)\n",
    "print(f\"  User 101 Status: {validation_result['status']}\")\n",
    "print(f\"  Months Available: {validation_result['months_available']}\")\n",
    "print(f\"  Transaction Count: {validation_result['transaction_count']}\")\n",
    "print(f\"  Data Valid: {validation_result['is_valid']}\")\n",
    "\n",
    "print(\"\\n[DEMO] Goal Feasibility Analysis (with validation & audit):\")\n",
    "feasibility = analyze_goal_feasibility(user_id=101, target_amount=5000, months_to_deadline=6)\n",
    "if feasibility['status'] == STATUS_CODES['SUCCESS']:\n",
    "    print(f\"  [OK] Goal: ${feasibility['target_amount']:.2f}\")\n",
    "    print(f\"  [OK] Feasibility: {feasibility['feasibility_score']:.2f} ({feasibility['completion_confidence']})\")\n",
    "    print(f\"  [OK] Status: {feasibility['goal_status']}\")\n",
    "    print(f\"  [OK] Monthly Net Savings: ${feasibility['avg_monthly_net_savings']:.2f}\")\n",
    "    print(f\"  [OK] Projected Savings: ${feasibility['projected_savings']:.2f}\")\n",
    "    print(f\"  [OK] Shortfall: ${feasibility['projected_shortfall']:.2f}\")\n",
    "    if feasibility['audit_id']:\n",
    "        print(f\"  [OK] Audit logged (ID: {feasibility['audit_id'][:20]}...)\")\n",
    "else:\n",
    "    print(f\"  ERROR Status: {feasibility['status']}\")\n",
    "    print(f\"  ERROR Message: {feasibility['message']}\")\n",
    "    if 'audit_id' in feasibility:\n",
    "        print(f\"  ERROR Audit logged (ID: {feasibility['audit_id'][:20]}...)\")\n",
    "\n",
    "print(\"\\n[OK] Optimized Feedback Loop & Goal Tracking System READY!\")\n",
    "print(\"[OK] With Data Validation & Admin Audit Logging for Compliance\")\n",
    "print(\"[OK] Multi-Class Category Classifier Support with Corrected Training Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No spending-over-income alerts for this month.\n"
     ]
    }
   ],
   "source": [
    "# Spending Exceeds Income Alert (Monthly)\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def trigger_alert(level, message, user_id=None, total_income=None, total_expenses=None):\n",
    "    severity = 'HIGH' if level == 'CRITICAL' else ('MEDIUM' if level == 'WARNING' else 'LOW')\n",
    "    extra = \"\"\n",
    "    if (total_income is not None) and (total_expenses is not None):\n",
    "        try:\n",
    "            extra = f\" | income=${float(total_income):.2f}, expenses=${float(total_expenses):.2f}\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        _uid = user_id if user_id is not None else -1\n",
    "        log_audit_entry(\n",
    "            user_id=_uid,\n",
    "            decision_type='budget_risk',\n",
    "            severity=severity,\n",
    "            trigger_reason=f\"{message}{extra}\",\n",
    "            model_confidence_score=None,\n",
    "            model_used='spending_vs_income_alert'\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f\"[ALERT:{level}] {message}{extra}\")\n",
    "\n",
    "now = datetime.now()\n",
    "month_start = datetime(now.year, now.month, 1)\n",
    "next_month = (month_start + timedelta(days=32)).replace(day=1)\n",
    "\n",
    "_date_col = None\n",
    "for col in ['transaction_date', 'date', 'timestamp', 'created_at']:\n",
    "    if col in data_clean.columns:\n",
    "        _date_col = col\n",
    "        break\n",
    "\n",
    "if _date_col:\n",
    "    dfm = data_clean.copy()\n",
    "    dfm['_date_parsed'] = pd.to_datetime(dfm[_date_col], errors='coerce')\n",
    "    dfm = dfm[(dfm['_date_parsed'] >= month_start) & (dfm['_date_parsed'] < next_month)]\n",
    "else:\n",
    "    dfm = data_clean.copy()\n",
    "\n",
    "_amount_col = None\n",
    "for col in ['amount', 'transaction_amount', 'value', 'amt']:\n",
    "    if col in dfm.columns:\n",
    "        _amount_col = col\n",
    "        break\n",
    "\n",
    "if _amount_col is None:\n",
    "    numeric_cols = dfm.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numeric_cols) > 0:\n",
    "        _amount_col = numeric_cols[0]\n",
    "\n",
    "if _amount_col is None or len(dfm) == 0:\n",
    "    print(\"No numeric transaction amount column found or no transactions for this month.\")\n",
    "else:\n",
    "    user_ids = dfm['user_id'].unique().tolist() if 'user_id' in dfm.columns else [None]\n",
    "\n",
    "    def _income_expense(dfu):\n",
    "        if 'transaction_type' in dfu.columns:\n",
    "            t = dfu['transaction_type'].astype(str).str.lower()\n",
    "            income_mask = t.str.contains('income')\n",
    "            expense_mask = t.str.contains('expense')\n",
    "            total_income = float(dfu.loc[income_mask, _amount_col].sum())\n",
    "            total_expenses = float(dfu.loc[expense_mask, _amount_col].abs().sum())\n",
    "        else:\n",
    "            amounts = dfu[_amount_col].dropna()\n",
    "            total_income = float(amounts[amounts > 0].sum())\n",
    "            total_expenses = float(abs(amounts[amounts < 0].sum()))\n",
    "        return total_income, total_expenses\n",
    "\n",
    "    alerts_triggered = 0\n",
    "    for uid in user_ids:\n",
    "        dfu = dfm if uid is None else dfm[dfm['user_id'] == uid]\n",
    "        income, expenses = _income_expense(dfu)\n",
    "        if expenses > income and (income + expenses) > 0:\n",
    "            trigger_alert(\n",
    "                'CRITICAL',\n",
    "                'Total spending has exceeded total income for this month',\n",
    "                user_id=uid,\n",
    "                total_income=income,\n",
    "                total_expenses=expenses\n",
    ")\n",
    "            alerts_triggered += 1\n",
    "\n",
    "    if alerts_triggered == 0:\n",
    "        print(\"No spending-over-income alerts for this month.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OOP Refactored Model Architecture Demo\n",
      "======================================================================\n",
      "[TIME] Data Loading: 61.9ms\n",
      "\n",
      "[1/4] Data loaded: 8000 rows\n",
      "\n",
      "[2/4] Training CategorizeModel...\n",
      "  Categorizer trained: {'ok': False, 'error': 'Model not compiled'}\n",
      "\n",
      "[3/4] Training FraudDetectionModel...\n",
      "  FraudDetector trained: {'ok': False, 'error': 'Model not compiled'}\n",
      "\n",
      "[4/4] Testing GoalTrackingModel goal feasibility...\n",
      "[TIME] Goal Feasibility: 1.2ms\n",
      "  User 1 goal feasibility: {'status': 'ERR_INVALID_USER', 'user_id': 1, 'feasibility_score': None, 'message': 'User not found in transaction history'}\n",
      "\n",
      "[AUDIT LOG] Total entries: 3\n",
      "[FEEDBACK LOG] Total entries: 0\n",
      "\n",
      "[OK] OOP Architecture Demo Complete - All models inherit from BaseAIModel\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"OOP Refactored Model Architecture Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cfg = AppConfig()\n",
    "engine = FinanceAIEngine(cfg)\n",
    "engine.load_data(cfg.train_dataset_path or 'final_train_dataset.csv')\n",
    "print(f\"\\n[1/4] Data loaded: {len(engine.data)} rows\")\n",
    "\n",
    "print(f\"\\n[2/4] Training CategorizeModel...\")\n",
    "x_dummy = np.random.rand(100, 20).astype('float32')\n",
    "y_dummy = np.eye(10)[np.random.randint(0, 10, 100)]\n",
    "engine.categorizer.build_network(input_shape=20)\n",
    "result = engine.categorizer.train_with_early_stopping(x_dummy, y_dummy, epochs=5, patience=2)\n",
    "print(f\"  Categorizer trained: {result}\")\n",
    "if result.get('ok'):\n",
    "    plot_file = engine.categorizer.plot_loss_history(save_path='categorizer_loss.png')\n",
    "    print(f\"  Plot saved: {plot_file}\")\n",
    "\n",
    "print(f\"\\n[3/4] Training FraudDetectionModel...\")\n",
    "y_fraud = np.random.randint(0, 2, 100).reshape(-1, 1).astype('float32')\n",
    "engine.fraud_detector.build_network(input_shape=20)\n",
    "result = engine.fraud_detector.train_with_early_stopping(x_dummy, y_fraud, epochs=5, patience=2)\n",
    "print(f\"  FraudDetector trained: {result}\")\n",
    "if result.get('ok'):\n",
    "    plot_file = engine.fraud_detector.plot_loss_history(save_path='fraud_loss.png')\n",
    "    print(f\"  Plot saved: {plot_file}\")\n",
    "\n",
    "print(f\"\\n[4/4] Testing GoalTrackingModel goal feasibility...\")\n",
    "sample_user = int(engine.data['user_id'].iloc[0]) if 'user_id' in engine.data.columns and len(engine.data) > 0 else 1\n",
    "feasibility = engine.goal_tracker.predict_feasibility(sample_user, engine.data, target_amount=5000, months_to_deadline=6)\n",
    "print(f\"  User {sample_user} goal feasibility: {feasibility}\")\n",
    "\n",
    "print(f\"\\n[AUDIT LOG] Total entries: {len(engine.audit.df)}\")\n",
    "print(f\"[FEEDBACK LOG] Total entries: {len(engine.feedback.df)}\")\n",
    "print(\"\\n[OK] OOP Architecture Demo Complete - All models inherit from BaseAIModel\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
